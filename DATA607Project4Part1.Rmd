---
title: "DATA 607 - Project 5 - Document Classifier"
author: "Project 4 Team: Banu Boopalan, Samuel Kigamba, James Mundy, Alain T Kuiete "
output:
  html_document:
    css:
    highlight: pygments
    theme: cerulean
    toc: true
    toc_float: true
  pdf_document: default
---

## Our Approach

Our approach for this project follows:

1. Load required Libraies
2. Get data from spamassassin website
3. Build a Build a Document Corpus
4. Create Document-Term Matrix
5. Clean-up and Normalize Data
6. Create Traing Set
7. Build/Train SVM
8. Review Results



```{r echo=TRUE, warning=FALSE, message=FALSE}
#loading required Libraries
library(caret)
library(dplyr)
library(tm)
library(e1071)
```

```{r}
library(tidyverse)
library(tidyr)
library(stringr)
library(tidytext)
```
## Get Data

#### The data for this project was obtained from: 

 https://spamassassin.apache.org/old/publiccorpus/
 
#### Ham and spam files were extracted and stored in a data folder on a local drive.

## Analysing the Ham files

### Downloading the Dataset for Ham

### Creating Ham Data Frame
```{r}

ham.dir="C:\\DATA607\\Project4\\spamHam\\20021010_easy_ham (1).tar\\easy_ham"
ham.file.names = list.files(ham.dir)

# List of docs
ham.docs <- ham.file.names[1]
for(i in 2:length(ham.file.names))
{
  filepath<-paste0(ham.dir, "/", ham.file.names[i])  
  text <-readLines(filepath)
  list1<- list(paste(text, collapse="\n"))
  ham.docs = c(ham.docs,list1)
}
```

### Extracting the Ham senders emails

```{r}
senders <- unlist(str_extract(ham.docs[2], "(?<name>[\\w.-]+)\\@(?<domain>[-\\w+\\.\\w+]+)(\\.\\w+)?"))
for (i  in 3:length(ham.docs)) {
  s <- unlist(str_extract(ham.docs[i],"(?<name>[\\w.-]+)\\@(?<domain>[-\\w+\\.\\w+]+)(\\.\\w+)?"))
  senders <- c(senders, s)  
}
#summary(senders)
#head(senders)
```

### Creating a Ham Sender' Email Data Frame
```{r}
email.length <- nchar(senders[1])
for (i in 2:length(senders)) {
  email.length <-c(email.length,nchar(senders[i]))
}
sender.df <- tibble(email=senders, length=email.length)
#sender.df
```

### vizualizing the Length of Different Senders' Emails
```{r}
boxplot(sender.df$length)
```


### Grouping the Senders' emails by email address
```{r}
sen.email <- sender.df %>% 
  group_by( new.email =email, length)%>%
  summarise(n=n())%>%
  arrange(desc(n))
```
 

### visualizing the 10 most frequent Emails Ham  
```{r}
sender.df %>%
  group_by(email) %>%
  summarise(n=n())%>%
  top_n(10)%>%
  mutate(email = reorder(email, n)) %>%
  ggplot(aes(email, n, fill = email)) +
  geom_col(show.legend = FALSE)  +
  labs(y = "Most Frequent Senders",
       x = NULL) +
  coord_flip()
```


### Example of a Ham File
```{r}
#ham.docs[4]
```

### Using Regular Expressions to extract all the emails in the Ham Files
```{r}
emails <- unlist(str_extract_all(ham.docs[2],"(?<name>[\\w.-]+)\\@(?<domain>[-\\w+\\.\\w+]+)(\\.\\w+)?"))
for (i  in 3:length(ham.docs)) {
  s <- unlist(str_extract_all(ham.docs[i],"(?<name>[\\w.-]+)\\@(?<domain>[-\\w+\\.\\w+]+)(\\.\\w+)?"))
  emails <- c(emails, s)  
}
summary(emails)
```

### Turning These Ham Emails to a Data Frame
```{r}
len <- nchar(emails[1])
for (i in 2:length(emails)) {
  len <-c(len, nchar(emails[i]))
}
ham.emails <- tibble(mail = 1:length(emails), emails, len)
#ham.emails
```

### visualizing the length of all Emails 
```{r}
boxplot(ham.emails$len)
```


### visualizing the 20 Most Frequent Emails
```{r}
ham.emails %>%
  group_by(emails) %>%
  summarise(n=n())%>%
  top_n(20)%>%
  mutate(emails = reorder(emails, n)) %>%
  ggplot(aes(emails, n, fill = emails)) +
  geom_col(show.legend = FALSE)  +
  labs(y = "Most Frequent emails",
       x = NULL) +
  coord_flip()
```


## Body of the Email
### Extracting words in the Bodies of All Emails

###  Creating a Data Frame containing the words

```{r}
ham.list <- tibble(files = 1:length(ham.docs),
                   text = ham.docs)
```

### Adding the Frequence of Words to the Data frame
```{r}
ham.block <- ham.list %>%
  unnest_tokens(word, text)%>%
  group_by(files) %>%
  mutate(n= n()) %>%
  ungroup()
#ham.block
```


### Organizing the Data frame and adding the Term Frequency(tf), Inverse Document Frequency of a term(idf), and the combining of two term(tf_idf)
```{r}
ham.block <- ham.block %>%
  bind_tf_idf(word, files, n)
#ham.block %>%
 # arrange(desc(tf_idf))

```

### Cleaning the Data Frame, 
We select only words with IDF greater than 0 and we remove words containg numbers
```{r}
ham.block2 <- ham.block %>% 
  filter(idf>0,str_detect(word,"([^\\d.+\\w.+\\.\\,.+]+?)")) %>%
  arrange(desc(tf_idf))
#ham.block2
```

#### Example of the sparcity of a word
```{r}

filter(ham.block2, word=="laptop's")
```

### Visualization of the 20 Most Relevent Words in the Bodies of Emails
```{r}
ham.block2%>%
  arrange(desc(tf_idf)) %>%
  top_n(20)%>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  ggplot(aes(word, tf_idf, fill = files)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf", title = "Most Relevent Words in the Body Messages") +
  coord_flip()
```






## Spam Files

### Loading the Spam files

```{r}
spam.dir="C:\\DATA607\\Project4\\spamHam\\spam4\\spam_2"
spam.file.names = list.files(spam.dir)

spam_files = list.files(path = ham.dir, full.names = TRUE)
no_of_spam_files = length(list.files(spam.dir, all.files = "FALSE", full.names = "TRUE"))
print(paste("There are",no_of_spam_files,"spam emails in the spam_2 folder"))
#spam_files

# List of docs
spam.docs <- spam.file.names[1]
for(i in 2:length(spam.file.names))
{
  filepath<-paste0(spam.dir, "\\", spam.file.names[i])  
  text <-readLines(filepath)
  l<- list(paste(text, collapse="\n"))
  spam.docs = c(spam.docs,l)
}
```

```{r}

spam.dir="C:\\DATA607\\Project4\\spamHam\\spam4\\spam_2"
spam.file.names = list.files(spam.dir)

# List of docs
spam.docs <- spam.file.names[1]
for(i in 2:length(spam.file.names))
{
  filepath<-paste0(spam.dir, "\\", spam.file.names[i])  
  text <-readLines(filepath)
  l<- list(paste(text, collapse="\n"))
  spam.docs = c(spam.docs,l)
}
```

### Example of a Spam Document
```{r}
#spam.docs[7]
```

### Creating Spam Dataframe
```{r}
spam.list <- tibble(block = 1:length(spam.docs),
                   text = spam.docs)
```

### Extracting Word from The Bodies of Spam Files
```{r}
spam.block <- spam.list %>%
  unnest_tokens(word, text)%>%
  group_by(block) %>%
  mutate(n= n()) %>%
  ungroup()

```

### Selecting the Most Frequent Words with TF_IDF
```{r}
spam.block <- spam.block %>%
  bind_tf_idf(word, block, n)
#spam.block %>%
 # arrange(desc(tf_idf))

```


### Cleaning The Spam List of Words
```{r}
spam.block2 <- spam.block %>% 
  filter(idf>0,str_detect(word,"([^\\d.+\\w.+\\.\\,.+]+?)")) %>%
  arrange(desc(tf_idf))
#spam.block2
```



### Creating a Spam Sender' Email Data Frame
```{r}
spam.senders <- unlist(str_extract(spam.docs[2], "(?<name>[\\w.-]+)\\@(?<domain>[-\\w+\\.\\w+]+)(\\.\\w+)?"))
for (i  in 3:length(spam.docs)) {
  s <- unlist(str_extract(spam.docs[i],"(?<name>[\\w.-]+)\\@(?<domain>[-\\w+\\.\\w+]+)(\\.\\w+)?"))
  spam.senders <- c(spam.senders, s)  
}
summary(spam.senders)
head(spam.senders)
```

### Creating  a Spam Senders' Email Data Frame
```{r}
spam.email.len <- nchar(spam.senders[1])
for (i in 2:length(spam.senders)) {
  spam.email.len <-c(spam.email.len,nchar(spam.senders[i]))
}
spam.sender.df <- tibble(email=spam.senders, len=spam.email.len)
spam.sender.df
```

### vizualizing the Length of Different Spam Senders' Emails
```{r}
boxplot(spam.sender.df$len)
```


### Grouping the Spam Senders' emails by email address
```{r}
spam.sen.email <- spam.sender.df %>% 
  group_by( new.email =email, len)%>%
  summarise(n=n())%>%
  arrange(desc(n))
```
 
### vizualizing the 10 Most Relevent Spam Senders' Emails 
```{r}
spam.sender.df %>%
  group_by(email) %>%
  summarise(n=n())%>%
  top_n(10)%>%
  mutate(email = reorder(email, n)) %>%
  ggplot(aes(email, n, fill = email)) +
  geom_col(show.legend = FALSE)  +
  labs(y = "Most Frequent Senders",
       x = NULL) +
  coord_flip()
```


### Example of Spam Document
```{r}
#spam.docs[2]
```


```{r}
spam.emails <- unlist(str_extract_all(spam.docs[2],"(?<name>[\\w.-]+)\\@(?<domain>[-\\w+\\.\\w+]+)(\\.\\w+)?"))
for (i  in 3:length(spam.docs)) {
  s <- unlist(str_extract_all(spam.docs[i],"(?<name>[\\w.-]+)\\@(?<domain>[-\\w+\\.\\w+]+)(\\.\\w+)?"))
  spam.emails <- c(spam.emails, s)  
}
summary(spam.emails)
```

### vizualizing the Length of Different Senders' Emails
```{r}
len <- nchar(spam.emails[1])
for (i in 2:length(spam.emails)) {
  len <-c(len, nchar(spam.emails[i]))
}
spam.emails <- tibble(mail = 1:length(spam.emails), spam.emails, len)
#spam.emails
```


```{r}
boxplot(spam.emails$len)
```



```{r}
spam.emails %>%
  group_by(spam.emails) %>%
  summarise(n=n())%>%
  top_n(20)%>%
  mutate(spam.emails = reorder(spam.emails, n)) %>%
  ggplot(aes(spam.emails, n, fill = spam.emails)) +
  geom_col(show.legend = FALSE)  +
  labs(y = "Most Frequent emails",
       x = NULL) +
  coord_flip()
```



### Visualization of the 10 Most Relevent Words in the Bodies of Spam Emails
```{r}
spam.block2%>%
  top_n(10)%>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  mutate(block = reorder(block, tf_idf)) %>%
  arrange(desc(tf_idf)) %>%
  ggplot(aes(word, tf_idf, fill = block)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf", title = "Most Relevent Words in the Bodies of Spam Email") +
  coord_flip()
```





## Build a Corpus 

#### Next we build the corpus after completing some transforms: convert to plain doucment, remove stopwords, remove punctuation, remove numbers, remove whitespace, etc.

```{r echo=TRUE, warning=FALSE, message=FALSE}
create_corpus <- function(dir, label){
  corpus <- VCorpus(DirSource(dir)) %>%
    tm_map(PlainTextDocument)  %>%
    tm_map(content_transformer(tolower)) %>% # 
    tm_map(removeWords, stopwords("SMART")) %>% 
    tm_map(removePunctuation) %>% # 
    tm_map(removeNumbers) %>% # 
    tm_map(stripWhitespace) %>% # 
    tm_map(stemDocument) # 
  meta(corpus, "LABEL") <- label
  return(corpus)
}
#data/easy_ham_2 <- "C:\\DATA607\\Project4\\spamHam\\20021010_easy_ham (1).tar\\easy_ham"
#data/spam_2<- "C:\\DATA607\\Project4\\spamHam\\20021010_spam.tar\\spam"
corpus<- c(create_corpus("C:\\DATA607\\Project4\\spamHam\\spam3\\spam", "Spam"), create_corpus("C:\\DATA607\\Project4\\spamHam\\20021010_easy_ham (1).tar\\easy_ham", "Ham"))
```



## Build a Document-Term Matrix

#### Now we use the corpus to construct a document term matrix.  

```{r echo=TRUE, warning=FALSE, message=FALSE}

dtm <- DocumentTermMatrix(corpus)

```


### Reduce Sparseness and Normalize

#### We reduce sparness here by only keeping words that are found more than n times. We tried training the model with differnt values for n but found that 15 produced the best results . 


```{r echo=TRUE, warning=FALSE, message=FALSE }
#Only Keep Words found in at least 15 documents

min_docs <- 15
dtm <- removeSparseTerms(dtm, 1 - (min_docs / length(corpus)))

model_data <- as.matrix(dtm)
words <- rowSums(model_data)
model_data <- model_data / words
model_data <- data.frame(model_data)
model_data <- cbind(meta(corpus), model_data) %>%
  mutate(LABEL = as.factor(LABEL))
```

## Create a Training Set

#### We now divide the data into training and test sets.  Seventy-five percent of the data was used for training.

```{r  echo=TRUE, warning=FALSE, message=FALSE}
set.seed(12345)
in_training_set <- createDataPartition(model_data$LABEL, p = 0.75,  list = FALSE)
training_data <- model_data[in_training_set, ]
testing_data <- model_data[-in_training_set, ]
```

## Build / Train SVM

#### We use the training data to build a SVM model that predicts if a message is spam or ham.

```{r echo=TRUE, warning=FALSE, message=FALSE}
model <- svm(LABEL ~ ., data = training_data)
```

## Review Results

#### Finally, we test our model to see how accurate it is.

```{r echo=TRUE, warning=FALSE, message=FALSE}
predictions <- testing_data %>%
  select(-LABEL) %>%
  predict(model, .)
```

#### The confusion matrix below indicates that with n = 15, only 8 emails were misclassified. This equates to approximately 99% accuracy. 


```{r}
#length(predictions)
#length(testing_data$LABEL)
```

```{r echo=TRUE, warning=FALSE, message=FALSE}
library(kableExtra)
table(predictions, testing_data$LABEL) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "responsive"))
```

